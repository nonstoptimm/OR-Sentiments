xgb_fit <- xgboost(data = train_matrix, params = xgb_params, nrounds = 20)
# Check the feature importance
importance_vars <- xgb.importance(model=xgb_fit, feature_names = colnames(train_matrix))
head(importance_vars, 20)
# Try to plot a partial dependency plot of one of the features
partial(xgb_fit, train = movie_review, pred.var = "bad")
str(prep_headphone_brand$scoreNN)
xgb_params = list(
objective = "reg:linear",
eta = 0.01,
max.depth = 5,
eval_metric = "auc")
xgb_fit <- xgboost(data = train_matrix, params = xgb_params, nrounds = 20)
# Check the feature importance
importance_vars <- xgb.importance(model=xgb_fit, feature_names = colnames(train_matrix))
importance_vars
prep_headphone_brand$review
# Try to plot a partial dependency plot of one of the features
partial(xgb_fit, train = movie_review, pred.var = "bad")
importance_vars
xgb_fit <- xgboost(data = train_matrix, booster = "gblinear", params = xgb_params, nrounds = 20)
xgb_fit <- xgboost(data = train_matrix,
objective = "reg:linear",
eval_metric = "rmse",
max.depth =15,
eta = 0.1,
nround = 15,
subsample = 0.5,
colsample_bytree = 0.5,
num_class = 12,
nthread = 3
)
xgb_fit <- xgboost(data = train_matrix,
objective = "reg:linear",
label = prep_headphone_brand$scoreNN
eval_metric = "rmse",
max.depth =15,
eta = 0.1,
nround = 15,
subsample = 0.5,
colsample_bytree = 0.5,
num_class = 12,
nthread = 3
)
xgb_fit <- xgboost(data = train_matrix, booster = "gblinear", params = xgb_params, nrounds = 20)
xgb_fit
importance_vars <- xgb.importance(model=xgb_fit, feature_names = colnames(train_matrix))
head(importance_vars, 20)
head(importance_vars, 100)
# Try to plot a partial dependency plot of one of the features
partial(xgb_fit, train = movie_review, pred.var = "bad")
importance_vars %>% arrange(Weight)
?arrange
xgb.plot.importance(importance_vars, top_n = 20)
?xgboost
# Model
xgb.fit <- xgboost(data = train_matrix, label = XSalePrice,
booster = "gbtree", objective = "reg:linear",
colsample_bytree = 0.2, gamma = 0.0,
learning_rate = 0.05, max_depth = 6,
min_child_weight = 1.5, n_estimators = 7300,
reg_alpha = 0.9, reg_lambda = 0.5,
subsample = 0.2, seed = 42,
silent = 1, nrounds = 25)
xgb.fit <- xgboost(data = train_matrix, label = prep_headphone_brand$scoreNN,
booster = "gbtree", objective = "reg:linear",
colsample_bytree = 0.2, gamma = 0.0,
learning_rate = 0.05, max_depth = 6,
min_child_weight = 1.5, n_estimators = 7300,
reg_alpha = 0.9, reg_lambda = 0.5,
subsample = 0.2, seed = 42,
silent = 1, nrounds = 25)
xgb.fit <- xgboost(data = train_matrix,
booster = "gbtree", objective = "reg:linear",
colsample_bytree = 0.2, gamma = 0.0,
learning_rate = 0.05, max_depth = 6,
min_child_weight = 1.5, n_estimators = 7300,
reg_alpha = 0.9, reg_lambda = 0.5,
subsample = 0.2, seed = 42,
silent = 1, nrounds = 25)
xgb_fit <- xgboost(data = train_matrix, booster = "gblinear", params = xgb_params, nrounds = 20)
importance_vars <- xgb.importance(model=xgb_fit, feature_names = colnames(train_matrix))
head(importance_vars, 100)
xgb.plot.importance(importance_vars, top_n = 20)
xgb.fit <- xgboost(data = train_matrix,
booster = "gbtree", objective = "reg:linear",
colsample_bytree = 0.2, gamma = 0.0,
learning_rate = 0.05, max_depth = 6,
min_child_weight = 1.5, n_estimators = 7300,
reg_alpha = 0.9, reg_lambda = 0.5,
subsample = 0.2, seed = 42,
silent = 1, nrounds = 25)
dtm_train <- create_dtm(itoken(prep_headphone_brand$review,
preprocessor = tolower,
tokenizer = word_tokenizer),
vocab_vectorizer(vocab))
# Turn the DTM into an XGB matrix using the sentiment labels that are to be learned
train_matrix <- xgb.DMatrix(dtm_train, label = prep_headphone_brand$scoreNN)
xgb.fit <- xgboost(data = train_matrix,
booster = "gbtree", objective = "reg:linear",
colsample_bytree = 0.2, gamma = 0.0,
learning_rate = 0.05, max_depth = 6,
min_child_weight = 1.5, n_estimators = 7300,
reg_alpha = 0.9, reg_lambda = 0.5,
subsample = 0.2, seed = 42,
silent = 1, nrounds = 25)
# Check the feature importance
importance_vars <- xgb.importance(model=xgb_fit, feature_names = colnames(train_matrix))
head(importance_vars, 100)
is.numeric(prep_headphone_brand$scoreNN)
getSEnti
#
train <- merged_topic_headphone[1:100000]
train <- merged_topic_headphone[1:100000,]
test <- merged_topic_headphone[100001:219372,]
makeFeatures <- function(train) {
labeledTerms = makeDTM(train)
## Preparing the features for the XGBoost Model
features <- colnames(labeledTerms)
for (f in features) {
if ((class(labeledTerms[[f]])=="factor") || (class(labeledTerms[[f]])=="character")) {
levels <- unique(labeledTerms[[f]])
labeledTerms[[f]] <- as.numeric(factor(labeledTerms[[f]], levels=levels))
}
}
return(labeledTerms)
}
labeledTerms = makeFeatures(train)
library(text2vec)
library(xgboost)
library(pdp)
# Create the document term matrix (bag of words) using the movie_review data frame provided
# in the text2vec package (sentiment analysis problem)
#data("movie_review")
train <- prep_headphone_brand[1:10000,]
test <- prep_headphone_brand[10001:12000,]
?xgboost
xgb_params = list(
objective = "reg:linear",
eta = 0.01,
max.depth = 1000)
bound <- floor(nrow(train) * 0.9)
train <- train[1:bound, ]
df.train <- train[1:bound, ]
df.validation <- train[(bound+1):nrow(train),]
vocab <- create_vocabulary(itoken(train$review,
preprocessor = tolower,
tokenizer = word_tokenizer))
bound <- floor(nrow(train) * 0.9)
train <- train[1:bound, ]
df.train <- train[1:bound, ]
df.validation <- train[(bound+1):nrow(train),]
train.y <- df.train$scoreNN
validation.y <- df.validation$scoreNN
dtm_train <- create_dtm(itoken(df.train$review,
preprocessor = tolower,
tokenizer = word_tokenizer),
vocab_vectorizer(vocab))
dtm_test <- create_dtm(itoken(df.validation$review,
preprocessor = tolower,
tokenizer = word_tokenizer),
vocab_vectorizer(vocab))
# Turn the DTM into an XGB matrix using the sentiment labels that are to be learned
train_matrix <- xgb.DMatrix(dtm_train, label = train$scoreNN)
train_matrix <- xgb.DMatrix(dtm_train, label = train.y)
test_matrix <- xgb.DMatrix(dtm_test, label = validation.y)
watchlist <- list(validation = test_matrix, train=train_matrix)
xgb_fit <- xgboost(data = train_matrix, params = xgb_params, watchlist = watchlist, early.stop.round = 20, maximise = false, nrounds = 1000)
xgb_fit <- xgboost(data = train_matrix, params = xgb_params, watchlist = watchlist, early.stop.round = 20, maximize = false, nrounds = 1000)
# xgboost model building
xgb_params = list(
objective = "reg:linear",
eta = 0.01,
max.depth = 1000)
xgb_fit <- xgboost(data = train_matrix, params = xgb_params, watchlist = watchlist, early.stop.round = 20, maximize = false, nrounds = 1000)
train_matrix <- xgb.DMatrix(dtm_train, label = train.y)
test_matrix <- xgb.DMatrix(dtm_test, label = validation.y)
watchlist <- list(validation = test_matrix, train=train_matrix)
xgb_params = list(
objective = "reg:linear",
eta = 0.01,
max.depth = 1000)
xgb_fit <- xgboost(data = train_matrix, params = xgb_params, watchlist = watchlist, early.stop.round = 20, maximize = false, nrounds = 1000)
xgb_fit <- xgboost(params = xgb_params, data = train_matrix, nrounds = 1000, watchlist = watchlist, early_stopping_rounds = 20, maximize = FALSE)
xgb_fit <- xgb.train(params = xgb_params, data = train_matrix, nrounds = 1000, watchlist = watchlist, early_stopping_rounds = 20, maximize = FALSE)
# Validation
bound <- floor(nrow(prep_headphone_brand) * 0.9)
train <- train[1:bound, ]
df.train <- train[1:bound, ]
df.validation <- train[(bound+1):nrow(train),]
train.y <- df.train$scoreNN
validation.y <- df.validation$scoreNN
vocab <- create_vocabulary(itoken(train$review,
preprocessor = tolower,
tokenizer = word_tokenizer))
dtm_train <- create_dtm(itoken(df.train$review,
preprocessor = tolower,
tokenizer = word_tokenizer),
vocab_vectorizer(vocab))
dtm_test <- create_dtm(itoken(df.validation$review,
preprocessor = tolower,
tokenizer = word_tokenizer),
vocab_vectorizer(vocab))
train_matrix <- xgb.DMatrix(dtm_train, label = train.y)
test_matrix <- xgb.DMatrix(dtm_test, label = validation.y)
watchlist <- list(validation = test_matrix, train=train_matrix)
View(xgb_params)
# Validation
bound <- floor(nrow(prep_headphone_brand) * 0.9)
train <- train[sample(nrow(train)), ]
df.train <- train[1:bound, ]
df.validation <- train[(bound+1):nrow(train),]
train.y <- df.train$scoreNN
validation.y <- df.validation$scoreNN
bound <- floor(nrow(prep_headphone_brand) * 0.9)
bound <- floor(nrow(prep_headphone_brand) * 0.9)
train <- train[sample(nrow(train)), ]
df.train <- train[1:bound, ]
df.validation <- train[(bound+1):nrow(train),]
df.validation
df.train
head(prep_headphone_brand)
is.na(prep_headphone_brand$asin)
# Create the document term matrix (bag of words) using the movie_review data frame provided
# in the text2vec package (sentiment analysis problem)
#data("movie_review")
train <- prep_headphone_brand[1:10000,]
test <- prep_headphone_brand[10001:12000,]
train.y <- train$scoreNN
validation.y <- test$scoreNN
dtm_train <- create_dtm(itoken(train$review,
preprocessor = tolower,
tokenizer = word_tokenizer),
vocab_vectorizer(vocab))
vocab <- create_vocabulary(itoken(train$review,
preprocessor = tolower,
tokenizer = word_tokenizer))
dtm_train <- create_dtm(itoken(train$review,
preprocessor = tolower,
tokenizer = word_tokenizer),
vocab_vectorizer(vocab))
dtm_test <- create_dtm(itoken(test$review,
preprocessor = tolower,
tokenizer = word_tokenizer),
vocab_vectorizer(vocab))
# Turn the DTM into an XGB matrix using the sentiment labels that are to be learned
train_matrix <- xgb.DMatrix(dtm_train, label = train.y)
test_matrix <- xgb.DMatrix(dtm_test, label = validation.y)
watchlist <- list(validation = test_matrix, train=train_matrix)
xgb_fit <- xgb.train(params = xgb_params, data = train_matrix, nrounds = 1000, watchlist = watchlist, early_stopping_rounds = 20, maximize = FALSE)
predict(xgb_fit, test)
predict(xgb_fit, test$scoreNN)
saveLDA(LDA_reviews_headphone, "LDA_reviews_headphone")
rm(LDA_reviews_headphone)
set.seed(101)
# Take a small sample of the data
sample <- sample.int(n = nrow(prep_headphone_brand), size = floor(.3*nrow(prep_headphone_brand)), replace = F)
# Pull sample from dataset
mediate <- prep_headphone_brand[sample, ]
# Create train and test dataset
train <- mediate[1:50000,]
test <- mediate[50001:63111,]
# Create vocabulary
# Tokenize the movie reviews and create a vocabulary of tokens including document counts
vocab <- create_vocabulary(itoken(mediate$review,
tokenizer = word_tokenizer))
library(text2vec)
library(xgboost)
library(pdp)
# Build a document-term matrix using the tokenized review text. This returns a dgCMatrix object
dtm_train <- create_dtm(itoken(train$review, tokenizer = word_tokenizer),
vocab_vectorizer(vocab))
dtm_test <- create_dtm(itoken(test$review,
preprocessor = tolower,
tokenizer = word_tokenizer),
vocab_vectorizer(vocab))
vocab <- create_vocabulary(itoken(mediate$review,
tokenizer = word_tokenizer))
# Build a document-term matrix using the tokenized review text. This returns a dgCMatrix object
dtm_train <- create_dtm(itoken(train$review, tokenizer = word_tokenizer),
vocab_vectorizer(vocab))
dtm_test <- create_dtm(itoken(test$review,
preprocessor = tolower,
tokenizer = word_tokenizer),
vocab_vectorizer(vocab))
train.y <- train$scoreNN
validation.y <- test$scoreNN
train_matrix <- xgb.DMatrix(dtm_train, label = train.y)
test_matrix <- xgb.DMatrix(dtm_test, label = validation.y)
watchlist <- list(validation = test_matrix, train=train_matrix)
xgb_params = list(
objective = "reg:linear",
eta = 0.01,
max.depth = 1000)
rm(xgb_fit)
xgb_fit <- xgb.train(params = xgb_params, data = train_matrix, nrounds = 10000, watchlist = watchlist, early_stopping_rounds = 20, maximize = FALSE)
saveLDA(xgb_fit, "xgb_fit")
# CLEANUP - this command was sometimes really necessary as we're dealing with a huge amount of data
rm(list=ls())
# IMPORT THE DATA
xbg_fit <- readRDS("output/xgb_params.rds")
# IMPORT THE DATA
xbg_fit <- readRDS("output/xgb_fit.rds")
# Check the feature importance
importance_vars <- xgb.importance(model=xgb_fit, feature_names = colnames(train_matrix))
# Check the feature importance
importance_vars <- xgb.importance(model=xgb_fit, feature_names = colnames(train_matrix))
rm(xbg_fit)
# IMPORT THE DATA
xgb_fit <- readRDS("output/xgb_fit.rds")
# Check the feature importance
importance_vars <- xgb.importance(model=xgb_fit, feature_names = colnames(train_matrix))
prep_headphone_brand  <- fread("input-data/prep_headphone_brand.csv")
library(data.table)
prep_headphone_brand  <- fread("input-data/prep_headphone_brand.csv")
# IMPORT THE CREATED TOPIC MODELS
LDA_reviews_cellphone <- readRDS("output/LDA_reviews_cellphone.rds")
rm(LDA_reviews_cellphone)
# Set seed for reproducible sampling
set.seed(101)
# Take a small sample of the data
sample <- sample.int(n = nrow(prep_headphone_brand), size = floor(.3*nrow(prep_headphone_brand)), replace = F)
# Pull sample from dataset
mediate <- prep_headphone_brand[sample, ]
# Create train and test dataset
train <- mediate[1:50000,]
test <- mediate[50001:63111,]
# Create vocabulary
# Tokenize the movie reviews and create a vocabulary of tokens including document counts
vocab <- create_vocabulary(itoken(mediate$review,
tokenizer = word_tokenizer))
# Build a document-term matrix using the tokenized review text. This returns a dgCMatrix object
dtm_train <- create_dtm(itoken(train$review, tokenizer = word_tokenizer),
vocab_vectorizer(vocab))
dtm_test <- create_dtm(itoken(test$review,
preprocessor = tolower,
tokenizer = word_tokenizer),
vocab_vectorizer(vocab))
# Train and Test Scores
train.y <- train$scoreNN
validation.y <- test$scoreNN
# Turn the DTM into an XGB matrix using the sentiment labels that are to be learned
train_matrix <- xgb.DMatrix(dtm_train, label = train.y)
test_matrix <- xgb.DMatrix(dtm_test, label = validation.y)
# Create a watchlist
watchlist <- list(validation = test_matrix, train=train_matrix)
# Check the feature importance
importance_vars <- xgb.importance(model=xgb_fit, feature_names = colnames(train_matrix))
head(importance_vars, 100)
library(tidytext)
stop_words
stop_words %>% filter(word == "only")
stop_words %>% filter(word == "a")
library(dplyr)
stop_words %>% filter(word == "a")
stop_words %>% filter(word == "a")
stop_words %>% filter(word == "only")
saveData(importance_vars, "importance_vars")
saveData <- function(input, filename){
fwrite(input, paste("output/", filename, ".csv", sep=""))
}
saveData(importance_vars, "importance_vars")
?create_dtm
?itoken
vocab <- create_vocabulary(itoken(train$review,
preprocessor = tolower,
tokenizer = word_tokenizer, stopwords = stop_words))
dtm_train
summary(dtm_train)
dtm_train <- create_dtm(itoken(train$review,
preprocessor = tolower,
tokenizer = word_tokenizer),
vocab_vectorizer(vocab))
dtm_test <- create_dtm(itoken(test$review,
preprocessor = tolower,
tokenizer = word_tokenizer),
vocab_vectorizer(vocab))
train_matrix <- xgb.DMatrix(dtm_train, label = train.y)
test_matrix <- xgb.DMatrix(dtm_test, label = validation.y)
watchlist <- list(validation = test_matrix, train=train_matrix)
vocab <- create_vocabulary(itoken(mediate$review,
tokenizer = word_tokenizer,
stopwords = stop_words))
# Build a document-term matrix using the tokenized review text. This returns a dgCMatrix object
dtm_train <- create_dtm(itoken(train$review, tokenizer = word_tokenizer),
vocab_vectorizer(vocab))
dtm_test <- create_dtm(itoken(test$review,
preprocessor = tolower,
tokenizer = word_tokenizer),
vocab_vectorizer(vocab))
# Train and Test Scores
train.y <- train$scoreNN
validation.y <- test$scoreNN
train_matrix <- xgb.DMatrix(dtm_train, label = train.y)
test_matrix <- xgb.DMatrix(dtm_test, label = validation.y)
# Create a watchlist
watchlist <- list(validation = test_matrix, train=train_matrix)
xgb_params = list(
objective = "reg:linear",
eta = 0.01,
max.depth = 1000)
rm(xgb_fit)
xgb_fit <- xgb.train(params = xgb_params, data = train_matrix, nrounds = 10000, watchlist = watchlist, early_stopping_rounds = 20, maximize = FALSE)
# Define individual stopwords
stopwordsIndividual <- c("a",	"about",	"again",	"against",	"all",	"am",	"an",	"and",	"any",	"are",	"aren't",	"as",	"at",	"be",	"because",	"been",	"before",	"being",	"below",	"between",	"both",	"but",	"by",	"can't",	"cannot",	"could",	"couldn't",	"did",	"didn't",	"do",	"does",	"doesn't",	"doing",	"don't",	"down",	"during",	"each",	"few",	"for",	"from",	"further",	"had",	"hadn't",	"has",	"hasn't",	"have",	"haven't",	"having",	"he",	"he'd",	"he'll",	"he's",	"her",	"here",	"here's",	"hers",	"herself",	"him",	"himself",	"his",	"how",	"how's",	"i",	"i'd",	"i'll",	"i'm",	"i've",	"if",	"in",	"into",	"is",	"isn't",	"it",	"it's",	"its",	"itself",	"let's",	"me",	"more",	"most",	"mustn't",	"my",	"myself",	"of",	"off",	"on",	"once",	"only",	"or",	"ought",	"our",	"ours",	"ourselves",	"out",	"over",	"own",	"same",	"shan't",	"she",	"she'd",	"she'll",	"she's",	"should",	"shouldn't",	"so",	"some",	"such",	"than",	"that",	"that's",	"the",	"their",	"theirs",	"them",	"themselves",	"then",	"there",	"there's",	"these",	"they",	"they'd",	"they'll",	"they're",	"they've",	"this",	"those",	"through",	"to",	"until",	"up",	"very",	"was",	"wasn't",	"we",	"we'd",	"we'll",	"we're",	"we've",	"were",	"weren't",	"what",	"what's",	"when",	"when's",	"where",	"where's",	"which",	"while",	"who",	"who's",	"whom",	"why",	"why's",	"with",	"won't",	"would",	"wouldn't",	"you",	"you'd",	"you'll",	"you're",	"you've",	"your",	"yours",	"yourself",	"yourselves")
# Create train and test dataset
train <- mediate[1:50000,]
test <- mediate[50001:63111,]
# Create vocabulary
# Tokenize the movie reviews and create a vocabulary of tokens including document counts
vocab <- create_vocabulary(itoken(mediate$review,
tokenizer = word_tokenizer,
stopwords = stopwordsIndividual))
# Build a document-term matrix using the tokenized review text. This returns a dgCMatrix object
dtm_train <- create_dtm(itoken(train$review, tokenizer = word_tokenizer),
vocab_vectorizer(vocab))
dtm_test <- create_dtm(itoken(test$review,
preprocessor = tolower,
tokenizer = word_tokenizer),
vocab_vectorizer(vocab))
# Train and Test Scores
train.y <- train$scoreNN
validation.y <- test$scoreNN
# Turn the DTM into an XGB matrix using the sentiment labels that are to be learned
train_matrix <- xgb.DMatrix(dtm_train, label = train.y)
test_matrix <- xgb.DMatrix(dtm_test, label = validation.y)
# Create a watchlist
watchlist <- list(validation = test_matrix, train=train_matrix)
# xgboost model building
xgb_params = list(
objective = "reg:linear",
eta = 0.01,
max.depth = 1000)
xgb_fit <- xgb.train(params = xgb_params, data = train_matrix, nrounds = 10000, watchlist = watchlist, early_stopping_rounds = 20, maximize = FALSE)
saveLDA(xgb_fit, "xgb_fit1")
saveLDA <- function(input, filename) {
saveRDS(input, paste("output/", filename, ".rds", sep=""))
}
saveLDA(xgb_fit, "xgb_fit1")
# CLEANUP - this command was sometimes really necessary as we're dealing with a huge amount of data
rm(list=ls())
# OBJECT WRITER
library(data.table)
xgb_fit <- readRDS("/Volumes/OMEGA/Dataset/prepared_data/XGBOOST/xgb_fit_noProgress.rds")
xgb_fit <- readRDS("/Volumes/OMEGA/Dataset/prepared_data/XGBOOST/xgb_fit_noProgress.rds")
xgb_fit <- readRDS("/Volumes/OMEGA/Dataset/prepared_data/XGBOOST/xgb_fit_noProgress.rds")
xgb_fit <- readRDS("output/xgb_fit_noProgress.rds")
xgb_fit <- readRDS("/Volumes/OMEGA/Dataset/prepared_data/XGBOOST/xgb_fit-noProgress.rds")
prep_headphone_brand  <- fread("output/prep_headphone_brand-clean.csv")
# XGBOOST CLASSIFIER #
library(text2vec)
library(xgboost)
library(pdp)
library(dplyr)
library(tidytext)
library(tidyverse)
set.seed(101)
# Take a small sample of the data
sample <- sample.int(n = nrow(prep_headphone_brand), size = floor(.4*nrow(prep_headphone_brand)), replace = F)
# Pull sample from dataset
mediate <- prep_headphone_brand[sample, ]
# Create individual stopword-list
stopword_list <- c("a",	"about",	"after",	"again",	"against",	"all",	"am",	"an",	"and",	"any",	"are",	"aren't",	"as",	"at",	"be",	"because",	"been",	"before",	"being",	"between",	"both",	"by",	"can't",	"cannot",	"could",	"couldn't",	"did",	"didn't",	"do",	"does",	"doesn't",	"doing",	"don't",	"down",	"during",	"each",	"for",	"from",	"further",	"had",	"hadn't",	"has",	"hasn't",	"have",	"haven't",	"having",	"he",	"he'd",	"he'll",	"he's",	"her",	"here",	"here's",	"hers",	"herself",	"him",	"himself",	"his",	"how",	"how's",	"i",	"i'd",	"i'll",	"i'm",	"i've",	"if",	"in",	"into",	"isn't",	"it's",	"its",	"itself",	"let's",	"me",	"more",	"most",	"mustn't",	"my",	"myself",	"nor",	"only",	"other",	"ought",	"our",	"ours",	"ourselves",	"out",	"over",	"own",	"same",	"shan't",	"she",	"she'd",	"she'll",	"she's",	"should",	"shouldn't",	"so",	"some",	"such",	"that",	"that's",	"the",	"their",	"theirs",	"them",	"themselves",	"then",	"there",	"there's",	"these",	"they",	"they'd",	"they'll",	"they're",	"they've",	"this",	"those",	"through",	"too",	"under",	"until",	"up",	"very",	"was",	"wasn't",	"we",	"we'd",	"we'll",	"we're",	"we've",	"were",	"weren't",	"what",	"what's",	"when",	"when's",	"where",	"where's",	"which",	"while",	"who",	"who's",	"whom",	"why",	"why's",	"with",	"won't",	"would",	"wouldn't",	"you",	"you'd",	"you'll",	"you're",	"you've",	"your",	"yours",	"yourself",	"yourselves")
# Create tibbles, so that it can be proceeded by dplyr and the unnest/nest/anti-join-functions
stopword_list <- as.tibble(stopword_list)
mediate <- as.tibble(mediate)
mediate <- mediate %>%
unnest_tokens(word, review) %>% # unnest the reviews to single words
anti_join(stopword_list, by = c("word" = "value")) %>% # anti-join to predefined stopword list
nest(word) %>% # nest the words again to list items
mutate(review = map(data, unlist), # create review column and glue the terms together again
review = map_chr(review, paste, collapse = " ")) # separate by a blank
# Create train and test dataset
train <- mediate[1:50000,] # training-data
test <- mediate[50001:62187,] # test-dataset
test2 <- mediate[62188:84148,]
# Create vocabulary
# Tokenize the movie reviews and create a vocabulary of tokens including document counts
vocab <- create_vocabulary(itoken(mediate$review,
tokenizer = word_tokenizer))
# Build a document-term matrix using the tokenized review text. This returns a dgCMatrix object
dtm_train <- create_dtm(itoken(train$review,
tokenizer = word_tokenizer), vocab_vectorizer(vocab))
dtm_test <- create_dtm(itoken(test$review,
tokenizer = word_tokenizer), vocab_vectorizer(vocab))
# Train and Test Scores
train.y <- train$scoreNN
validation.y <- test$scoreNN
#validation2.y <- test2$scoreNN
# Turn the DTM into an XGB matrix using the sentiment labels that are to be learned
train_matrix <- xgb.DMatrix(dtm_train, label = train.y)
importance_vars2 <- xgb.importance(model=xgb_fit, feature_names = colnames(train_matrix))
saveData(importance_vars, "importance_vars2-noProgress")
# PREPARED DATA
saveData <- function(input, filename){
fwrite(input, paste("output/", filename, ".csv", sep=""))
}
saveData(importance_vars, "importance_vars2-noProgress")
saveData(importance_vars2, "importance_vars2-noProgress")
# Print most important Features
head(importance_vars2, 110)
# Print most important Features
head(importance_vars2, 100)
xgb_fit
# CLEANUP - this command was sometimes really necessary as we're dealing with a huge amount of data
rm(list=ls())
# IMPORT THE DATA
xgb_fit <- readRDS("/Volumes/OMEGA/Dataset/prepared_data/XGBOOST/xgb_fit_noStopwords.rds")
# IMPORT THE DATA
xgb_fit <- readRDS("output/xgb_fit_noStopwords.rds")
xgb_fit
rm(list=ls())
