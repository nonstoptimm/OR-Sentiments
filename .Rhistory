# IMPORT THE CREATED TOPIC MODELS
LDA_reviews_cellphone <- readRDS("output/LDA_reviews_cellphone.rds")
rm(LDA_reviews_cellphone)
# Set seed for reproducible sampling
set.seed(101)
# Take a small sample of the data
sample <- sample.int(n = nrow(prep_headphone_brand), size = floor(.3*nrow(prep_headphone_brand)), replace = F)
# Pull sample from dataset
mediate <- prep_headphone_brand[sample, ]
# Create train and test dataset
train <- mediate[1:50000,]
test <- mediate[50001:63111,]
# Create vocabulary
# Tokenize the movie reviews and create a vocabulary of tokens including document counts
vocab <- create_vocabulary(itoken(mediate$review,
tokenizer = word_tokenizer))
# Build a document-term matrix using the tokenized review text. This returns a dgCMatrix object
dtm_train <- create_dtm(itoken(train$review, tokenizer = word_tokenizer),
vocab_vectorizer(vocab))
dtm_test <- create_dtm(itoken(test$review,
preprocessor = tolower,
tokenizer = word_tokenizer),
vocab_vectorizer(vocab))
# Train and Test Scores
train.y <- train$scoreNN
validation.y <- test$scoreNN
# Turn the DTM into an XGB matrix using the sentiment labels that are to be learned
train_matrix <- xgb.DMatrix(dtm_train, label = train.y)
test_matrix <- xgb.DMatrix(dtm_test, label = validation.y)
# Create a watchlist
watchlist <- list(validation = test_matrix, train=train_matrix)
# Check the feature importance
importance_vars <- xgb.importance(model=xgb_fit, feature_names = colnames(train_matrix))
head(importance_vars, 100)
library(tidytext)
stop_words
stop_words %>% filter(word == "only")
stop_words %>% filter(word == "a")
library(dplyr)
stop_words %>% filter(word == "a")
stop_words %>% filter(word == "a")
stop_words %>% filter(word == "only")
saveData(importance_vars, "importance_vars")
saveData <- function(input, filename){
fwrite(input, paste("output/", filename, ".csv", sep=""))
}
saveData(importance_vars, "importance_vars")
?create_dtm
?itoken
vocab <- create_vocabulary(itoken(train$review,
preprocessor = tolower,
tokenizer = word_tokenizer, stopwords = stop_words))
dtm_train
summary(dtm_train)
dtm_train <- create_dtm(itoken(train$review,
preprocessor = tolower,
tokenizer = word_tokenizer),
vocab_vectorizer(vocab))
dtm_test <- create_dtm(itoken(test$review,
preprocessor = tolower,
tokenizer = word_tokenizer),
vocab_vectorizer(vocab))
train_matrix <- xgb.DMatrix(dtm_train, label = train.y)
test_matrix <- xgb.DMatrix(dtm_test, label = validation.y)
watchlist <- list(validation = test_matrix, train=train_matrix)
vocab <- create_vocabulary(itoken(mediate$review,
tokenizer = word_tokenizer,
stopwords = stop_words))
# Build a document-term matrix using the tokenized review text. This returns a dgCMatrix object
dtm_train <- create_dtm(itoken(train$review, tokenizer = word_tokenizer),
vocab_vectorizer(vocab))
dtm_test <- create_dtm(itoken(test$review,
preprocessor = tolower,
tokenizer = word_tokenizer),
vocab_vectorizer(vocab))
# Train and Test Scores
train.y <- train$scoreNN
validation.y <- test$scoreNN
train_matrix <- xgb.DMatrix(dtm_train, label = train.y)
test_matrix <- xgb.DMatrix(dtm_test, label = validation.y)
# Create a watchlist
watchlist <- list(validation = test_matrix, train=train_matrix)
xgb_params = list(
objective = "reg:linear",
eta = 0.01,
max.depth = 1000)
rm(xgb_fit)
xgb_fit <- xgb.train(params = xgb_params, data = train_matrix, nrounds = 10000, watchlist = watchlist, early_stopping_rounds = 20, maximize = FALSE)
# Define individual stopwords
stopwordsIndividual <- c("a",	"about",	"again",	"against",	"all",	"am",	"an",	"and",	"any",	"are",	"aren't",	"as",	"at",	"be",	"because",	"been",	"before",	"being",	"below",	"between",	"both",	"but",	"by",	"can't",	"cannot",	"could",	"couldn't",	"did",	"didn't",	"do",	"does",	"doesn't",	"doing",	"don't",	"down",	"during",	"each",	"few",	"for",	"from",	"further",	"had",	"hadn't",	"has",	"hasn't",	"have",	"haven't",	"having",	"he",	"he'd",	"he'll",	"he's",	"her",	"here",	"here's",	"hers",	"herself",	"him",	"himself",	"his",	"how",	"how's",	"i",	"i'd",	"i'll",	"i'm",	"i've",	"if",	"in",	"into",	"is",	"isn't",	"it",	"it's",	"its",	"itself",	"let's",	"me",	"more",	"most",	"mustn't",	"my",	"myself",	"of",	"off",	"on",	"once",	"only",	"or",	"ought",	"our",	"ours",	"ourselves",	"out",	"over",	"own",	"same",	"shan't",	"she",	"she'd",	"she'll",	"she's",	"should",	"shouldn't",	"so",	"some",	"such",	"than",	"that",	"that's",	"the",	"their",	"theirs",	"them",	"themselves",	"then",	"there",	"there's",	"these",	"they",	"they'd",	"they'll",	"they're",	"they've",	"this",	"those",	"through",	"to",	"until",	"up",	"very",	"was",	"wasn't",	"we",	"we'd",	"we'll",	"we're",	"we've",	"were",	"weren't",	"what",	"what's",	"when",	"when's",	"where",	"where's",	"which",	"while",	"who",	"who's",	"whom",	"why",	"why's",	"with",	"won't",	"would",	"wouldn't",	"you",	"you'd",	"you'll",	"you're",	"you've",	"your",	"yours",	"yourself",	"yourselves")
# Create train and test dataset
train <- mediate[1:50000,]
test <- mediate[50001:63111,]
# Create vocabulary
# Tokenize the movie reviews and create a vocabulary of tokens including document counts
vocab <- create_vocabulary(itoken(mediate$review,
tokenizer = word_tokenizer,
stopwords = stopwordsIndividual))
# Build a document-term matrix using the tokenized review text. This returns a dgCMatrix object
dtm_train <- create_dtm(itoken(train$review, tokenizer = word_tokenizer),
vocab_vectorizer(vocab))
dtm_test <- create_dtm(itoken(test$review,
preprocessor = tolower,
tokenizer = word_tokenizer),
vocab_vectorizer(vocab))
# Train and Test Scores
train.y <- train$scoreNN
validation.y <- test$scoreNN
# Turn the DTM into an XGB matrix using the sentiment labels that are to be learned
train_matrix <- xgb.DMatrix(dtm_train, label = train.y)
test_matrix <- xgb.DMatrix(dtm_test, label = validation.y)
# Create a watchlist
watchlist <- list(validation = test_matrix, train=train_matrix)
# xgboost model building
xgb_params = list(
objective = "reg:linear",
eta = 0.01,
max.depth = 1000)
xgb_fit <- xgb.train(params = xgb_params, data = train_matrix, nrounds = 10000, watchlist = watchlist, early_stopping_rounds = 20, maximize = FALSE)
saveLDA(xgb_fit, "xgb_fit1")
saveLDA <- function(input, filename) {
saveRDS(input, paste("output/", filename, ".rds", sep=""))
}
saveLDA(xgb_fit, "xgb_fit1")
# CLEANUP - this command was sometimes really necessary as we're dealing with a huge amount of data
rm(list=ls())
# OBJECT WRITER
library(data.table)
xgb_fit <- readRDS("/Volumes/OMEGA/Dataset/prepared_data/XGBOOST/xgb_fit_noProgress.rds")
xgb_fit <- readRDS("/Volumes/OMEGA/Dataset/prepared_data/XGBOOST/xgb_fit_noProgress.rds")
xgb_fit <- readRDS("/Volumes/OMEGA/Dataset/prepared_data/XGBOOST/xgb_fit_noProgress.rds")
xgb_fit <- readRDS("output/xgb_fit_noProgress.rds")
xgb_fit <- readRDS("/Volumes/OMEGA/Dataset/prepared_data/XGBOOST/xgb_fit-noProgress.rds")
prep_headphone_brand  <- fread("output/prep_headphone_brand-clean.csv")
# XGBOOST CLASSIFIER #
library(text2vec)
library(xgboost)
library(pdp)
library(dplyr)
library(tidytext)
library(tidyverse)
set.seed(101)
# Take a small sample of the data
sample <- sample.int(n = nrow(prep_headphone_brand), size = floor(.4*nrow(prep_headphone_brand)), replace = F)
# Pull sample from dataset
mediate <- prep_headphone_brand[sample, ]
# Create individual stopword-list
stopword_list <- c("a",	"about",	"after",	"again",	"against",	"all",	"am",	"an",	"and",	"any",	"are",	"aren't",	"as",	"at",	"be",	"because",	"been",	"before",	"being",	"between",	"both",	"by",	"can't",	"cannot",	"could",	"couldn't",	"did",	"didn't",	"do",	"does",	"doesn't",	"doing",	"don't",	"down",	"during",	"each",	"for",	"from",	"further",	"had",	"hadn't",	"has",	"hasn't",	"have",	"haven't",	"having",	"he",	"he'd",	"he'll",	"he's",	"her",	"here",	"here's",	"hers",	"herself",	"him",	"himself",	"his",	"how",	"how's",	"i",	"i'd",	"i'll",	"i'm",	"i've",	"if",	"in",	"into",	"isn't",	"it's",	"its",	"itself",	"let's",	"me",	"more",	"most",	"mustn't",	"my",	"myself",	"nor",	"only",	"other",	"ought",	"our",	"ours",	"ourselves",	"out",	"over",	"own",	"same",	"shan't",	"she",	"she'd",	"she'll",	"she's",	"should",	"shouldn't",	"so",	"some",	"such",	"that",	"that's",	"the",	"their",	"theirs",	"them",	"themselves",	"then",	"there",	"there's",	"these",	"they",	"they'd",	"they'll",	"they're",	"they've",	"this",	"those",	"through",	"too",	"under",	"until",	"up",	"very",	"was",	"wasn't",	"we",	"we'd",	"we'll",	"we're",	"we've",	"were",	"weren't",	"what",	"what's",	"when",	"when's",	"where",	"where's",	"which",	"while",	"who",	"who's",	"whom",	"why",	"why's",	"with",	"won't",	"would",	"wouldn't",	"you",	"you'd",	"you'll",	"you're",	"you've",	"your",	"yours",	"yourself",	"yourselves")
# Create tibbles, so that it can be proceeded by dplyr and the unnest/nest/anti-join-functions
stopword_list <- as.tibble(stopword_list)
mediate <- as.tibble(mediate)
mediate <- mediate %>%
unnest_tokens(word, review) %>% # unnest the reviews to single words
anti_join(stopword_list, by = c("word" = "value")) %>% # anti-join to predefined stopword list
nest(word) %>% # nest the words again to list items
mutate(review = map(data, unlist), # create review column and glue the terms together again
review = map_chr(review, paste, collapse = " ")) # separate by a blank
# Create train and test dataset
train <- mediate[1:50000,] # training-data
test <- mediate[50001:62187,] # test-dataset
test2 <- mediate[62188:84148,]
# Create vocabulary
# Tokenize the movie reviews and create a vocabulary of tokens including document counts
vocab <- create_vocabulary(itoken(mediate$review,
tokenizer = word_tokenizer))
# Build a document-term matrix using the tokenized review text. This returns a dgCMatrix object
dtm_train <- create_dtm(itoken(train$review,
tokenizer = word_tokenizer), vocab_vectorizer(vocab))
dtm_test <- create_dtm(itoken(test$review,
tokenizer = word_tokenizer), vocab_vectorizer(vocab))
# Train and Test Scores
train.y <- train$scoreNN
validation.y <- test$scoreNN
#validation2.y <- test2$scoreNN
# Turn the DTM into an XGB matrix using the sentiment labels that are to be learned
train_matrix <- xgb.DMatrix(dtm_train, label = train.y)
importance_vars2 <- xgb.importance(model=xgb_fit, feature_names = colnames(train_matrix))
saveData(importance_vars, "importance_vars2-noProgress")
# PREPARED DATA
saveData <- function(input, filename){
fwrite(input, paste("output/", filename, ".csv", sep=""))
}
saveData(importance_vars, "importance_vars2-noProgress")
saveData(importance_vars2, "importance_vars2-noProgress")
# Print most important Features
head(importance_vars2, 110)
# Print most important Features
head(importance_vars2, 100)
xgb_fit
# CLEANUP - this command was sometimes really necessary as we're dealing with a huge amount of data
rm(list=ls())
# IMPORT THE DATA
xgb_fit <- readRDS("/Volumes/OMEGA/Dataset/prepared_data/XGBOOST/xgb_fit_noStopwords.rds")
# IMPORT THE DATA
xgb_fit <- readRDS("output/xgb_fit_noStopwords.rds")
xgb_fit
rm(list=ls())
prep_headphone_brand  <- as_tibble(fread("output/prep_headphone_brand-clean.csv"))
# SENTIMENT ANALYSIS ON ONLINE REVIEWS #
# dataImport.R
# LOAD PACKAGES
library(data.table)
library(dplyr)
prep_headphone_brand  <- as_tibble(fread("output/prep_headphone_brand-clean.csv"))
prep_cellphone_brand <- as_tibble(fread("output/prep_cellphone_brand-clean.csv"))
rm(prep_headphone_brand)
p <- ggplot(prep_cellphone_brand, aes(x=overall, y=scoreNN)) +
geom_boxplot()
library(ggplot2)
p <- ggplot(prep_cellphone_brand, aes(x=overall, y=scoreNN)) +
geom_boxplot()
View(p)
p
copy_cellphone
copy_cellphone <- prep_cellphone_brand
copy_cellphone$overall <- as.factor(copy_cellphone$overall)
levels(copy_cellphone)
levels(copy_cellphone$overall)
p <- ggplot(prep_cellphone_brand, aes(x=overall, y=scoreNN)) +
geom_boxplot()
View(p)
p <- ggplot(prep_cellphone_brand, aes(x=overall, y=scoreNN)) +
geom_boxplot()
p <- ggplot(copy_cellphone, aes(x=overall, y=scoreNN)) +
geom_boxplot()
View(p)
View(prep_cellphone_brand)
View(p)
View(p)
View(p)
View(p)
View(p)
View(p)
View(p)
ggplot(copy_cellphone, aes(x=overall, y=scoreNN)) +
geom_boxplot()
prep_headphone_brand  <- as_tibble(fread("output/prep_headphone_brand-clean.csv"))
prep_headphone_brand$overall <- as.factor(prep_headphone_brand$overall)
ggplot(prep_headphone_brand, aes(x=overall, y=scoreNN)) +
geom_boxplot()
x <- prep_cellphone_brand$scoreNN
h<-hist(x, breaks=10, col="red", xlab="Miles Per Gallon",
main="Histogram with Normal Curve")
xfit<-seq(min(x),max(x),length=40)
yfit<-dnorm(xfit,mean=mean(x),sd=sd(x))
yfit <- yfit*diff(h$mids[1:2])*length(x)
lines(xfit, yfit, col="blue", lwd=2)
x <- prep_headphone_brand$scoreNN
h<-hist(x, breaks=10, col="red", xlab="Miles Per Gallon",
main="Histogram with Normal Curve")
xfit<-seq(min(x),max(x),length=40)
yfit<-dnorm(xfit,mean=mean(x),sd=sd(x))
yfit <- yfit*diff(h$mids[1:2])*length(x)
lines(xfit, yfit, col="blue", lwd=2)
x <- prep_cellphone_brand$scoreNN
h<-hist(x, breaks=10, col="red", xlab="Miles Per Gallon",
main="Histogram with Normal Curve")
xfit<-seq(min(x),max(x),length=40)
yfit<-dnorm(xfit,mean=mean(x),sd=sd(x))
yfit <- yfit*diff(h$mids[1:2])*length(x)
lines(xfit, yfit, col="blue", lwd=2)
prep_coffee_brand <- as_tibble(fread("output/prep_coffee_brand-clean.csv"))
prep_toaster_brand  <- as_tibble(fread("output/prep_toaster_brand-clean.csv"))
x <- prep_cellphone_brand$scoreNN
h<-hist(x, breaks=10, col="red", xlab="Miles Per Gallon",
main="Histogram with Normal Curve")
xfit<-seq(min(x),max(x),length=10)
yfit<-dnorm(xfit,mean=mean(x),sd=sd(x))
yfit <- yfit*diff(h$mids[1:2])*length(x)
lines(xfit, yfit, col="blue", lwd=2)
x <- prep_cellphone_brand$scoreNN
h<-hist(x, breaks=100, col="red", xlab="Miles Per Gallon",
main="Histogram with Normal Curve")
xfit<-seq(min(x),max(x),length=40)
yfit<-dnorm(xfit,mean=mean(x),sd=sd(x))
yfit <- yfit*diff(h$mids[1:2])*length(x)
lines(xfit, yfit, col="blue", lwd=2)
d <- density(prep_cellphone_brand$scoreNN ) # returns the density data
plot(d)
x <- prep_cellphone_brand$scoreNN
h<-hist(x, breaks=100, col="red", xlab="Miles Per Gallon",
main="Histogram with Normal Curve")
xfit<-seq(min(x),max(x),length=40)
yfit<-dnorm(xfit,mean=mean(x),sd=sd(x))
yfit <- yfit*diff(h$mids[1:2])*length(x)
lines(xfit, yfit, col="blue", lwd=2)
d <- density(prep_cellphone_brand$scoreNN ) # returns the density data
plot(d)
d <- density(prep_headphone_brand$scoreNN ) # returns the density data
plot(d)
ks.test(prep_headphone_brand$scoreNN, "pnorm")
ks.test(prep_cellphone_brand$scoreNN, "pnorm")
ks.test(prep_toaster_brand$scoreNN, "pnorm")
prep_cellphone_brand %>% filter(overall == 1)
headphone_1 <- prep_cellphone_brand %>% filter(overall == 1)
d <- density(headphone_1$scoreNN) # returns the density data
plot(d)
ks.test(headphone_1$scoreNN, "pnorm")
?ks.test
ks.test(headphone_1$scoreNN, "dnorm")
ks.test(headphone_1$scoreNN, "pnorm")
5
test.score <- headphone_1$scoreNN + 5
summary(test.score)
ks.test(test.score, "pnorm")
ks.test(test.score,y='pnorm',alternative='two.sided')
install.packages(nortest)
install.packages"nortest"
install.packages("nortest")
library(nortest)
test.score <- headphone_1$scoreNN
ad.test(test.score)
headphone_5 <- prep_cellphone_brand %>% filter(overall == 5)
ad.test(headphone_5)
ad.test(headphone_5$scoreNN)
ks.test(headphone_5$scoreNN, "dnorm")
headphone_5 <- prep_cellphone_brand %>% filter(overall == 5) %>% select(scoreNN)
ks.test(headphone_5, "dnorm")
ks.test(headphone_5$scoreNN, "dnorm")
plot(d)
h<-hist(x, breaks=1000, col="red", xlab="Miles Per Gallon",
main="Histogram with Normal Curve")
xfit<-seq(min(x),max(x),length=40)
yfit<-dnorm(xfit,mean=mean(x),sd=sd(x))
yfit <- yfit*diff(h$mids[1:2])*length(x)
lines(xfit, yfit, col="blue", lwd=2)
d <- density(headphone_1$scoreNN) # returns the density data
plot(d)
d <- density(prep_headphone_brand$scoreNN) # returns the density data
plot(d)
d <- density(headphone_5$scoreNN) # returns the density data
plot(d)
headphone_3 <- prep_cellphone_brand %>% filter(overall == 3) %>% select(scoreNN)
d <- density(headphone_3$scoreNN) # returns the density data
plot(d)
ks.test(headphone_3$scoreNN, "dnorm")
kruskal.test(scoreNN ~ overall, prep_headphone_brand)
merged_topic_cellphone  <- as_tibble(fread("/Volumes/OMEGA/Dataset/prepared_data/old/merged_topic_cellphone.csv"))
# IMPORT THE CREATED TOPIC MODELS
LDA_reviews_cellphone <- readRDS("output/LDA_reviews_cellphone.rds")
# MAIN TOPIC OF EACH DOCUMENT
# probabilities associated with each topic assignment
mainTopic <- function(input){
matrix <- as.matrix(topics(input))
return(matrix)
}
# Apply mainTopic Function
LDA_main_cellphone <- mainTopic(LDA_reviews_cellphone)
LDA_main_cellphone
# Apply mainTopic Function
LDA_main_cellphone <- as.data.frame(mainTopic(LDA_reviews_cellphone))
LDA_main_cellphone
head(LDA_main_cellphone)
head(merged_topic_cellphone)
head(merged_topic_cellphone$document)
names(LDA_main_cellphone) <- c("document", "MainTopic")
# Apply mainTopic Function
LDA_main_cellphone <- as.matrxi(mainTopic(LDA_reviews_cellphone))
# Apply mainTopic Function
LDA_main_cellphone <- as.matrix(mainTopic(LDA_reviews_cellphone))
View(LDA_main_cellphone)
# Apply mainTopic Function
LDA_main_cellphone <- mainTopic(LDA_reviews_cellphone)
LDA_main_cellphone
names(LDA_main_cellphone)
# Apply mainTopic Function
LDA_main_cellphone <- as.data.table(mainTopic(LDA_reviews_cellphone))
LDA_main_cellphone
# Apply mainTopic Function
LDA_main_cellphone <- mainTopic(LDA_reviews_cellphone)
LDA_main_cellphone
str(LDA_main_cellphone)
unlist(LDA_main_cellphone)
LDA_cell <- unlist(LDA_main_cellphone)
LDA_cell
str(LDA_cell)
write.csv(LDA_main_cellphone, "LDAcell.csv")
write.csv(LDA_main_cellphone, "LDAcell.csv", sep=";")
fwrite(LDA_main_cellphone, "LDAcell.csv", sep =";")
fwrite(LDA_main_cellphone, "LDAcell.csv", sep=";")
library(data.table)
fwrite(LDA_main_cellphone, "LDAcell.csv", sep=";")
?write.csv
write.csv(LDA_main_cellphone, "LDAcell.csv", sep=";")
write.csv(LDA_main_cellphone, "LDAcell.csv")
fread("LDAcell.csv")
LDA_main_cellphone <- fread("LDAcell.csv")
View(LDA_main_cellphone)
LDA_main_cellphone <- fread("LDAcell.csv")
LDA_main_cellphone <- fread("LDAcell.csv")
View(LDA_main_cellphone)
?merge
LDA_cell <- merge(merged_topic_cellphone, LDA_main_cellphone, by="document")
LDA_cell
head(merged_topic_cellphone)
head(merged_topic_cellphone$document)
head(LDA_main_cellphone)
head(LDA_cell)
head(LDA_cell$document)
head(merged_topic_cellphone$document)
head(LDA_main_cellphone$document)
head(LDA_main_cellphone)
head(merged_topic_cellphone$MainTopic)
head(LDA_cell$MainTopic)
fwrite(LDA_cell, "LDA_cell.csv", sep=";")
ggplot(LDA_cell, aes(x=MainTopic, y=scoreNN)) +
geom_boxplot()
LDA_cell$MainTopic <- as.factor(LDA_cell$MainTopic)
ggplot(LDA_cell, aes(x=MainTopic, y=scoreNN)) +
geom_boxplot()
LDA_samsung <- LDA_cell %>% filter(brand == "samsung")
LDA_apple <- LDA_cell %>% filter(brand == "apple")
LDA_google <- LDA_cell %>% filter(brand == "google")
ggplot(LDA_google, aes(x=MainTopic, y=scoreNN)) +
geom_boxplot()
ggplot(LDA_apple, aes(x=MainTopic, y=scoreNN)) +
geom_boxplot()
ggplot(LDA_samsung, aes(x=MainTopic, y=scoreNN)) +
geom_boxplot()
LDA_lenovo <- LDA_cell %>% filter(brand == "lenovo")
ggplot(LDA_lenovo, aes(x=MainTopic, y=scoreNN)) +
geom_boxplot()
# Apply topic_wtp Function (LDA-Model as input)
LDA_cellphone_wtp <- topicWTP(LDA_reviews_cellphone)
# TIDY MODEL / WORD TOPIC PROBABILITIES
topicWTP <- function(input) {
tidy(input, matrix = "beta")
}
# Apply topic_wtp Function (LDA-Model as input)
LDA_cellphone_wtp <- topicWTP(LDA_reviews_cellphone)
library(tm)
# Apply topic_wtp Function (LDA-Model as input)
LDA_cellphone_wtp <- topicWTP(LDA_reviews_cellphone)
library(tidyverse)
library(tidytext)
# Apply topic_wtp Function (LDA-Model as input)
LDA_cellphone_wtp <- topicWTP(LDA_reviews_cellphone)
LDA_reviews_cellphone
LDA_reviews_cellphone@beta
# LDA TOP TERMS PER TOPIC
topicTopTerms <- function(input) {
input %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
}
# Apply LDATopTerms Function
LDA_cellphone_wtp_topTerms <- topicTopTerms(LDA_cellphone_wtp)
# Apply LDATopTerms Function
LDA_cellphone_wtp_topTerms <- topicTopTerms(LDA_cellphone_wtp)
# LDA TOP TERMS PER TOPIC
topicTopTerms <- function(input) {
input %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
}
# Apply LDATopTerms Function
LDA_cellphone_wtp_topTerms <- topicTopTerms(LDA_cellphone_wtp)
# TIDY MODEL / WORD TOPIC PROBABILITIES
topicWTP <- function(input) {
tidy(input, matrix = "beta")
}
# Apply topic_wtp Function (LDA-Model as input)
LDA_cellphone_wtp <- topicWTP(LDA_reviews_cellphone)
?tidy
library(purrr)
library(tidyverse)
library(tidytext)
# MAIN TOPIC OF EACH DOCUMENT
# probabilities associated with each topic assignment
mainTopic <- function(input){
matrix <- as.matrix(topics(input))
return(matrix)
}
# Apply mainTopic Function
LDA_main_cellphone <- mainTopic(LDA_reviews_cellphone)
# CREATE PROPER COLNAMES
# Create "Topic" colnames depending on the amount of topics
scoreCols <- function(input, num) {
names <- paste("Topic", 1:num, sep = "")
names(input) <- names
return(input)
}
# TIDY MODEL / WORD TOPIC PROBABILITIES
topicWTP <- function(input) {
tidy(input, matrix = "beta")
}
# Apply topic_wtp Function (LDA-Model as input)
LDA_cellphone_wtp <- topicWTP(LDA_reviews_cellphone)
# CLEANUP - this command was sometimes really necessary as we're dealing with a huge amount of data
rm(list=ls())
# IMPORT THE CREATED TOPIC MODELS
LDA_reviews_cellphone <- readRDS("output/LDA_reviews_cellphone.rds")
LDA_cell <- fread("LDA_cell.csv", sep=";")
library(data.table)
LDA_cell <- fread("LDA_cell.csv", sep=";")
library(topicmodels)
# generateTopics.R
# TOPIC MODELING
# Load required packages
library(dplyr)
library(tidyverse)
library(tidytext)
# TIDY MODEL / WORD TOPIC PROBABILITIES
topicWTP <- function(input) {
tidy(input, matrix = "beta")
}
# Apply topic_wtp Function (LDA-Model as input)
LDA_cellphone_wtp <- topicWTP(LDA_reviews_cellphone)
